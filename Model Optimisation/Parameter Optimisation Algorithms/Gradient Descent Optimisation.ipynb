{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimisation\n",
    "\n",
    "In this notebook we will be looking optimisation techniques that improve upon gradient descent which takes the following form: \n",
    "\n",
    "$$\\theta_{n+1}=\\theta-\\eta \\nabla_{\\theta}\\text{MSE}(\\theta)$$\n",
    "\n",
    "Where $\\nabla_{\\theta}\\text{MSE}(\\theta)$ is: \n",
    "\n",
    "$$\\nabla_{\\theta}\\text{MSE}(\\theta)=\\frac 2 m X^T(X\\theta-y)$$\n",
    "\n",
    "## Momentum\n",
    "\n",
    "To improve upon this algorithm momentum takes into account the weights from the previous iteration to remember the \"momentum\" of the descent: \n",
    "\n",
    "$$ \\Delta \\theta = \\alpha \\Delta \\theta - \\eta \\nabla_{\\theta}\\text{MSE}(\\theta)$$\n",
    "\n",
    "$$ \\theta = \\theta + \\Delta \\theta$$\n",
    "\n",
    "We implement it below using the gradient descent data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 + 3 * X + np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1 \n",
    "alpha = 0.1\n",
    "n_iterations = 1000 \n",
    "m = 100\n",
    "delta_theta = np.zeros((2,1))\n",
    "\n",
    "X_b = np.c_[ np.ones(( 100, 1)), X] # add bias vector of 1s\n",
    "\n",
    "thetas = np.random.randn(2,1) #random initialisation\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(thetas) - y)\n",
    "    delta_theta = alpha * delta_theta - eta * gradients\n",
    "    thetas += delta_theta\n",
    "thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get almost the same as the line $y=4+3x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "\n",
    "AdaGrad works by updating each parameter iteratively with a per-parameter learning rate however it still has the global learning rate $\\eta$. AdaGrad depends on the matrix $G$ which is the outer-product matrix: \n",
    "\n",
    "$$G = \\sum_{\\tau=1}^{t} g_\\tau g_\\tau^T$$\n",
    "\n",
    "Where $g_\\tau$ is the gradient at iteration $\\tau$. The formula for updating the weights then becomes:\n",
    "\n",
    "$$\\theta_{n+1} = \\theta - \\eta \\text{Diag}(G)^{-\\frac 1 2}\\odot g$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21500613],\n",
       "       [2.7701933 ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.2\n",
    "n_iterations = 10000 \n",
    "m = 100\n",
    "G = np.zeros((2,2))\n",
    "\n",
    "X_b = np.c_[ np.ones(( 100, 1)), X] # add bias vector of 1s\n",
    "\n",
    "thetas = np.random.randn(2,1) #random initialisation\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(thetas) - y)\n",
    "    G += gradients.dot(gradients.transpose())\n",
    "    thetas -= np.multiply(eta * np.power(G.diagonal().reshape((2,1)), -1/2), gradients)\n",
    "thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop\n",
    "\n",
    "For RMSProp different learning rates are used for each parameter. The formula is as follows:\n",
    "\n",
    "$$ v_{n+1} = \\gamma v + (1-\\gamma)(\\nabla_{\\theta}\\text{MSE}(\\theta))^2$$\n",
    "\n",
    "$$ \\theta_{n+1}=\\theta-\\frac{\\eta}{\\sqrt{v}}\\nabla_{\\theta}\\text{MSE}(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21559616],\n",
       "       [2.77061339]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.001\n",
    "n_iterations = 10000\n",
    "m = 100\n",
    "gamma = 0.1\n",
    "v = np.zeros((2,1))\n",
    "\n",
    "X_b = np.c_[ np.ones(( 100, 1)), X] # add bias vector of 1s\n",
    "\n",
    "thetas = np.random.randn(2,1) #random initialisation\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(thetas) - y)\n",
    "    v = gamma * v + (1-gamma) * np.power(gradients, 2)\n",
    "    thetas -= eta/np.power(v, 1/2)*gradients\n",
    "thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "Finally the most commonly used and arguably best optimiser is adam. Adam is similar to RMSprop but it also takes into account the second moments of the gradient which produces the following formulas:\n",
    "\n",
    "$$m_{t+1} = \\beta_1 m + (1-\\beta_1) \\nabla_{\\theta}\\text{MSE}(\\theta)$$ \n",
    "\n",
    "$$v_{t+1} = \\beta_2 v + (1-\\beta_2) (\\nabla_{\\theta}\\text{MSE}(\\theta))^2$$\n",
    "\n",
    "$$\\hat m_{t+1} = \\frac{m_{t+1}}{1-\\beta_1}$$\n",
    "\n",
    "$$\\hat v_{t+1} = \\frac{m_{t+1}}{1-\\beta_2}$$\n",
    "\n",
    "$$\\theta = \\theta - \\frac{\\eta \\hat m_{t+1}}{\\epsilon + \\sqrt{\\hat v_{t+1}}}$$\n",
    "\n",
    "Where epsilon is a small value to prevent division by 0 and beta is the forgetting factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 10 ** -8\n",
    "n_iterations = 1000\n",
    "n = 100\n",
    "m = np.zeros((2,1))\n",
    "v = np.zeros((2,1))\n",
    "\n",
    "X_b = np.c_[ np.ones(( 100, 1)), X] # add bias vector of 1s\n",
    "\n",
    "thetas = np.random.randn(2,1) #random initialisation\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/n * X_b.T.dot(X_b.dot(thetas) - y)\n",
    "    \n",
    "    m = beta1 * m + (1-beta1) * gradients\n",
    "    v = beta2 * v + (1-beta2) * np.power(gradients, 2)\n",
    "    m_hat = m/(1-beta1)\n",
    "    v_hat = m/(1-beta1)\n",
    "    \n",
    "    thetas -= eta*m_hat/(epsilon+np.power(v, 1/2))\n",
    "thetas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
