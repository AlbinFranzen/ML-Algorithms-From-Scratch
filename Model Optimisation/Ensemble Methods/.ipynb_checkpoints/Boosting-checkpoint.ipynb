{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Boosting are sequential ensemble methods that train predictors based on previous predictors. The two most common types are AdaBoost and Gradient Boosting.\n",
    "\n",
    "## AdaBoost (Adaptive Boosting)\n",
    "\n",
    "AdaBoost works by trying to pay more attention to instances that are underfit giving them higher weights. AdaBoost works by training a sequence of predictors where the weighted majority vote gives the final prediction. The weight for each predictor is called the *predictor rate* is calculated from the *weighted error rate*:\n",
    "\n",
    "$$ r_j = \\frac{\\underset{\\hat{y}_i=y_i}{\\sum_{i=1}^m w_i}}{\\sum_{i=1}^m w_i} $$\n",
    "\n",
    "$$\\alpha_j = \\eta \\log \\frac{1-r_j}{r_j} $$\n",
    "\n",
    "W is usually initialised as 1/m. To update the weights incorrect predictions are updated with $e^\\alpha_j$ and then normalised. To select the instances for the next predictor to train on it is done using roulette wheel selection based on the weights of each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(42)\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "dtc_clf = DecisionTreeClassifier(max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.5\n",
    "iterations = 20\n",
    "total = np.array([0.0 for i in range(len(X_test))])\n",
    "current_X = X_train\n",
    "current_y = y_train\n",
    "for i in range(iterations):\n",
    "    dtc_clf.fit(current_X, current_y)\n",
    "    y_pred = dtc_clf.predict(current_X) \n",
    "\n",
    "    weights = np.ones((len(X_train),1)).flatten() * 1/len(X_train)\n",
    "    weighted_error = ((y_pred != current_y) * weights).sum()\n",
    "    if weighted_error == 0:\n",
    "        weighted_error = np.amin(weights)/10\n",
    "    if weighted_error == 1:\n",
    "        weighted_error = 1-np.amin(weights)/10\n",
    "\n",
    "    alpha = eta * np.log((1-weighted_error)/weighted_error)\n",
    "    preds = dtc_clf.predict(X_test)\n",
    "    preds = np.where(preds==0, -1, preds)\n",
    "    total += preds * alpha\n",
    "\n",
    "    weights *= np.exp(np.where((y_pred != current_y)==0, -1, (y_pred != current_y))*alpha)\n",
    "    weights = weights/weights.sum()\n",
    "\n",
    "    current_X = np.empty((0,2))\n",
    "    current_y = np.array([])\n",
    "    for k in range(len(X_train)):\n",
    "        n = random.uniform(0,1)\n",
    "        temp_sum = 0\n",
    "        for m in range(len(weights)):\n",
    "            if temp_sum < n and n < (temp_sum + weights[m]):\n",
    "                current_X = np.vstack((current_X, X_train[m]))\n",
    "                current_y = np.hstack((current_y, y_train[m]))\n",
    "                break\n",
    "            temp_sum += weights[m]\n",
    "total = (total>=0)\n",
    "np.count_nonzero(total == y_test)/len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Similarly to AdaBoost gradient boost will add many predictors sequentially. However it does this by fitting predictors on the residuals of the previous predictors and adding the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 + 4 * X + 2 * X**2 + np.random.randn(100,1)\n",
    "\n",
    "data = np.hstack((X,y))\n",
    "temp = data[np.argsort(data[:,0])]\n",
    "X = temp[:,0].reshape((-1,1))\n",
    "y = temp[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Before boosting:\n",
      "13.531119142765169\n",
      "\n",
      "RMSE After Boosting\n",
      "11.93704629367871\n"
     ]
    }
   ],
   "source": [
    "dtr_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "dtr_reg1.fit(X,y)\n",
    "y_pred = dtr_reg1.predict(X)\n",
    "print(\"RMSE Before boosting:\")\n",
    "print(np.sqrt(sum((y_pred - y)**2)))\n",
    "\n",
    "y2 = y - dtr_reg1.predict(X)\n",
    "dtr_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "dtr_reg2.fit(X,y2)\n",
    "\n",
    "y3 = y - dtr_reg2.predict(X)\n",
    "dtr_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "dtr_reg3.fit(X,y3)\n",
    "\n",
    "y_pred = sum(dtr.predict(X) for dtr in (dtr_reg1, dtr_reg2, dtr_reg3))/2\n",
    "\n",
    "print(\"\")\n",
    "print(\"RMSE After Boosting\")\n",
    "print(np.sqrt(sum((y_pred - y)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code implements gradient boosting succesfully. A common method in gradient boosting is to use a shrinkage parameter where each tree becomes sequentially less important. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
