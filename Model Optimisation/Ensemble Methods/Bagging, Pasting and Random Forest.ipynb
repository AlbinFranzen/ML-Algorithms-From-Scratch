{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting\n",
    "\n",
    "Bagging and pasting are ensemble methods that use [resampling methods](https://github.com/AlbinFranzen/ML-Algorithms-From-Scratch/blob/master/Model%20Optimisation/Model%20Assessment/Resampling%20Methods.ipynb) to create new sets of data and train a model on the new sets, aggregating the results. Bagging is the same as the bootstrap which randomly  resamples. Pasting however will only use each each datapoint once for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc_clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: 0.864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dtc_clf.fit(X_train, y_train)\n",
    "dtc_clf_y_pred = dtc_clf.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree: \" + str(accuracy_score(y_test, dtc_clf_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = [0 for i in range(len(y_test))]\n",
    "for i in range(500):  \n",
    "    idx = np.array([np.random.randint(0,100) for i in range(100)])\n",
    "    resampled_x = X_train[idx]\n",
    "    resampled_y = y_train[idx]\n",
    "    dtc_clf.fit(resampled_x, resampled_y)\n",
    "    dtc_clf_y_pred = dtc_clf.predict(X_test)\n",
    "    total = [a + b for a, b in zip(total, dtc_clf_y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree with bagging: 0.88\n"
     ]
    }
   ],
   "source": [
    "final_guess = [0 if i<250 else 1 for i in total]\n",
    "final_guess = np.array(final_guess)\n",
    "print(\"Decision Tree with bagging: \" + str(accuracy_score(y_test, final_guess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total = [0 for i in range(len(y_test))]\n",
    "for i in range(500):  \n",
    "    idx = random.sample(range(70), 10)\n",
    "    resampled_x = X_train[idx]\n",
    "    resampled_y = y_train[idx]\n",
    "    dtc_clf.fit(resampled_x, resampled_y)\n",
    "    dtc_clf_y_pred = dtc_clf.predict(X_test)\n",
    "    total = [a + b for a, b in zip(total, dtc_clf_y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree with pasting: 0.84\n"
     ]
    }
   ],
   "source": [
    "final_guess = [0 if i<250 else 1 for i in total]\n",
    "final_guess = np.array(final_guess)\n",
    "print(\"Decision Tree with pasting: \" + str(accuracy_score(y_test, final_guess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Random forests are the same as bagging except now for each time the decision tree makes a split it can only do so using a random subset of all the predictors (subset size is usually equal to the square root of the number of predictors). This makes all the bagged trees less correlated and can generalise better to the dataset.\n",
    "\n",
    "## Extremly Randomized Trees (Extra-Trees)\n",
    "\n",
    "Extra trees are the same as random forests except for each except at each node instead of splitting based on criterion such as gini the splits are completely randomized. So now features are randomized, split is randomized and sampling is randomized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
