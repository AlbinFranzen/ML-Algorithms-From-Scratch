{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Error Estimation Performance Metrics\n",
    "\n",
    "When using traditional performance metrics (let's assume RMSE) to determine error of a training set for regression most often the model overfits the data and produces a lower RMSE for the training than the test set. To compensate for the systematic error of overfitting, changes to the performance metric can be made.\n",
    "\n",
    "## Mallow's $C_p$\n",
    "\n",
    "Mallows $C_p$ adjusts the RSS (recall RMSE = RSS $/n$) of a model with $d$ predictors using the variance $\\hat \\sigma$ of the predictions:\n",
    "\n",
    "$$C_p=\\frac 1 n (\\text{RSS}+2d\\hat{\\sigma}^2)$$\n",
    "\n",
    "Let's implement it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the data\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 + 3 * X + np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8065845639670532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "y_pred = lin_reg.predict(X)\n",
    "\n",
    "residuals = y_pred - y\n",
    "RSS = np.sum(residuals**2)\n",
    "RMSE = 1/len(y)*RSS #MSE formula\n",
    "print(\"RMSE: \" + str(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8227162552463942"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_C_p = 1/len(y)*(RSS+2*X.shape[1]*np.var(residuals))\n",
    "MSE_C_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akaike information criterion (AIC)\n",
    "\n",
    "AIC works by measuring information loss. If there are $k$ parameters and the maximum value of the log likelihood function is $\\hat{L}$ then the AIC is:\n",
    "\n",
    "$$\\text{AIC}=2k-2\\hat{L}$$\n",
    "\n",
    "A smaller AIC means more information per parameter which can be useful for measuring complexities of models. For ordinary least squares regression it can be shown the AIC is proportional to $C_p$ and has the value:\n",
    "\n",
    "$$\\text{AIC}=\\frac{1}{n\\hat \\sigma^2}(RSS+2d\\hat \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0199999999999998"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AIC = 1/(len(y)*np.var(residuals))*(RSS+2*X.shape[1]*np.var(residuals))\n",
    "AIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian information criterion (BIC)\n",
    "\n",
    "BIC is very similar to AIC but defines information loss as:\n",
    "\n",
    "$$\\text{BIC}=k\\ln n-2\\hat{L}$$\n",
    "\n",
    "And for ordinary least squares gives:\n",
    "\n",
    "$$\\text{BIC}=\\frac{1}{n\\hat \\sigma^2}(RSS+\\ln(n)d\\hat \\sigma^2)$$\n",
    "\n",
    "As we can see the $2$ is replaces by $ln(n)$ which places a penalty on models with many parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.046051701859881"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BIC = 1/(len(y)*np.var(residuals))*(RSS+np.log(len(y))*X.shape[1]*np.var(residuals))\n",
    "BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted $R^2$\n",
    "\n",
    "We know that the $R^2$ is defined as 1-RSS/TSS where TSS is the total sum of squares. However to adjust this to select models according to n and d we get:\n",
    "\n",
    "$$\\text{Adjusted }R^2=1-\\frac{\\text{RSS}/(n-d-1)}{\\text{TSS}/(n-1)}$$\n",
    "\n",
    "Unlike the previous accuracy metrics we want to maximise the adjusted $R^2$ instead of minimising it. Unlike the traditional $R^2$ the adjusted $R^2$ penalises unnecessary variables ($d$): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8805257289083981"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TSS = (y-(y-y_pred).mean()).sum()\n",
    "adjusted_R_squared = 1 - (RSS/(len(y)-1-1))/(TSS/(len(y)-1))\n",
    "adjusted_R_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
