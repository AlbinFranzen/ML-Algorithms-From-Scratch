{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Performance Metrics\n",
    "\n",
    "A common problem with machine learning algorithms is how to assess a model. This can be done with different performance metrics and most of them usually depend on the residual error:\n",
    "\n",
    "$$e_i=y_i-\\hat y_i$$\n",
    "\n",
    "### Root Mean Squared Error\n",
    "The most commonly used metric for regression is the *Root Mean Squared Error* (RMSE) which is defined as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}e_i^{2}}\n",
    "\\end{equation*}\n",
    "\n",
    "Below we have implemented RMSE on data modelled by a Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the data\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 + 3 * X + np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8981005311027564"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression() #apply model and make predictions\n",
    "lin_reg.fit(X, y)\n",
    "y_pred = lin_reg.predict(X)\n",
    "\n",
    "residuals = y_pred - y #calculate residuals\n",
    "root_mean_squared_error = np.sqrt(1/len(y)*np.sum(residuals**2)) #RMSE formula\n",
    "root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error\n",
    "\n",
    "The RMSE allows for convex solutions to many problems which is why it is generally preffered. Another metric that can be used is the *Mean Absolute Error* (MAE):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|e_i|\n",
    "\\end{equation*}\n",
    "\n",
    "We implement it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7010426719637758"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error = 1/len(y)*np.sum(abs(residuals)) #MAE formula\n",
    "mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above the MAE gives a lower error and this is due RMSE being more sensitive to outliers given that the residuals are squared. \n",
    "\n",
    "Both the RMSE and MAE can be used to measure distances between vectors. Various distance measures or *norms* can be used to do this:\n",
    "\n",
    "- The MAE is part of the $\\ell 1$ norm which is called *Manhattan norm* because it is the average residual\n",
    "\n",
    "- The RMSE is part of the $\\ell 1$ norm which is called *Euclidian norm* because it is the square root of the average square of residuals\n",
    "\n",
    "- Generally an $\\ell_k$ norm is nth root of the average n-exponent of residuals\n",
    "\n",
    "- Higher norm indexes focus more on large values and neglect small ones due to the exponents giving outliers more weight. RMSE is therefore ussually a reasonable choice as a prediction metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
